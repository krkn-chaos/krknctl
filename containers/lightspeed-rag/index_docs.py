#!/usr/bin/env python3
# Generated by Claude Code
# Documentation indexing script for krknctl Lightspeed RAG

import argparse
import os
import sys
import json
import logging
import time
import subprocess
import shutil
import tempfile
from typing import List, Dict, Any
from pathlib import Path

import numpy as np
from sentence_transformers import SentenceTransformer
import faiss

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DocumentationIndexer:
    def __init__(self, home_dir: str = "/app"):
        self.model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight embedding model
        self.documents = []
        self.embeddings = None
        self.index = None
        self.home_dir = home_dir
        
    def scrape_krkn_docs(self) -> List[Dict[str, Any]]:
        """Fetch documentation by cloning GitHub repository"""
        docs = []
        repo_url = "https://github.com/krkn-chaos/website.git"
        docs_path = "content/en/docs"
        
        try:
            # Clone repository to temporary directory
            docs = self._clone_and_extract_docs(repo_url, docs_path)
            logger.info(f"Found {len(docs)} documents from GitHub repository")
            
        except Exception as e:
            logger.error(f"Error during GitHub repository cloning: {e}")
            
        return docs
    
    def _clone_and_extract_docs(self, repo_url: str, docs_path: str) -> List[Dict[str, Any]]:
        """Clone repository and extract markdown files from docs directory"""
        docs = []
        
        with tempfile.TemporaryDirectory() as temp_dir:
            try:
                # Clone repository with minimal depth for efficiency
                logger.info(f"Cloning repository: {repo_url}")
                result = subprocess.run([
                    'git', 'clone', '--depth', '1', '--quiet', repo_url, temp_dir
                ], check=True, capture_output=True, text=True)
                
                # Debug: List what was cloned
                logger.info(f"Clone completed. Temp dir contents: {os.listdir(temp_dir)}")
                
                # Path to docs directory in cloned repo
                full_docs_path = os.path.join(temp_dir, docs_path)
                logger.info(f"Looking for docs at: {full_docs_path}")
                
                if not os.path.exists(full_docs_path):
                    logger.warning(f"Documentation path not found: {docs_path}")
                    logger.info(f"Available paths in temp_dir: {os.listdir(temp_dir)}")
                    return docs
                    
                # Recursively find all markdown files
                docs = self._extract_markdown_files(full_docs_path, docs_path)
                
            except subprocess.CalledProcessError as e:
                logger.error(f"Failed to clone repository: {e.stderr}")
                raise
            except Exception as e:
                logger.error(f"Error processing cloned repository: {e}")
                raise
                
        return docs
    
    def _extract_markdown_files(self, base_path: str, relative_docs_path: str) -> List[Dict[str, Any]]:
        """Recursively extract markdown files from directory"""
        docs = []
        
        try:
            logger.info(f"Extracting markdown files from: {base_path}")
            markdown_count = 0
            for root, dirs, files in os.walk(base_path):
                logger.info(f"Scanning directory: {root}, found {len(files)} files")
                for file in files:
                    if file.endswith('.md'):
                        markdown_count += 1
                        file_path = os.path.join(root, file)
                        logger.info(f"Processing markdown file: {file}")
                        doc = self._process_markdown_file(file_path, base_path, relative_docs_path)
                        if doc:
                            docs.append(doc)
            
            logger.info(f"Found {markdown_count} markdown files total, processed {len(docs)} successfully")
                            
        except Exception as e:
            logger.error(f"Error extracting markdown files: {e}")
            
        return docs
    
    def _process_markdown_file(self, file_path: str, base_path: str, relative_docs_path: str) -> Dict[str, Any]:
        """Process individual markdown file"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Get relative path from docs directory
            rel_path = os.path.relpath(file_path, base_path)
            
            # Parse frontmatter if present
            title = os.path.basename(file_path).replace('.md', '').replace('_', ' ').title()
            
            # Try to extract title from frontmatter
            if content.startswith('---'):
                try:
                    parts = content.split('---', 2)
                    if len(parts) >= 3:
                        frontmatter = parts[1]
                        content_body = parts[2].strip()
                        
                        # Extract title from frontmatter
                        for line in frontmatter.split('\n'):
                            if line.strip().startswith('title:'):
                                title = line.split(':', 1)[1].strip().strip('"\'')
                                break
                        
                        content = content_body
                except:
                    # If frontmatter parsing fails, use original content
                    pass
            
            # Generate URLs
            github_path = f"{relative_docs_path}/{rel_path}"
            github_url = f"https://github.com/krkn-chaos/website/blob/main/{github_path}"
            docs_url = f"https://krkn-chaos.dev/docs/{rel_path.replace('.md', '/')}"
            
            return {
                "url": docs_url,
                "title": title,
                "content": content,
                "source": "krkn-chaos/website",
                "github_url": github_url,
                "path": github_path
            }
            
        except Exception as e:
            logger.warning(f"Failed to process file {file_path}: {e}")
            return None
    
    def scrape_krkn_hub_scenarios(self) -> List[Dict[str, Any]]:
        """Fetch scenario definitions from krkn-hub repository"""
        docs = []
        repo_url = "https://github.com/krkn-chaos/krkn-hub.git"
        
        try:
            # Clone krkn-hub repository to temporary directory
            docs = self._clone_and_extract_scenarios(repo_url)
            logger.info(f"Found {len(docs)} scenario definitions from krkn-hub")
            
        except Exception as e:
            logger.error(f"Error during krkn-hub repository cloning: {e}")
            
        return docs
    
    def _clone_and_extract_scenarios(self, repo_url: str) -> List[Dict[str, Any]]:
        """Clone krkn-hub repository and extract scenario definitions"""
        docs = []
        
        with tempfile.TemporaryDirectory() as temp_dir:
            try:
                # Clone repository with minimal depth for efficiency
                logger.info(f"Cloning krkn-hub repository: {repo_url}")
                result = subprocess.run([
                    'git', 'clone', '--depth', '1', '--quiet', repo_url, temp_dir
                ], check=True, capture_output=True, text=True)
                
                # Find all krknctl-input.json files
                logger.info("Scanning for scenario definitions...")
                scenario_count = 0
                for root, dirs, files in os.walk(temp_dir):
                    if "krknctl-input.json" in files:
                        scenario_count += 1
                        scenario_name = os.path.basename(root)
                        json_path = os.path.join(root, "krknctl-input.json")
                        logger.info(f"Processing scenario: {scenario_name}")
                        doc = self._process_scenario_json(json_path, scenario_name)
                        if doc:
                            docs.append(doc)
                
                logger.info(f"Found {scenario_count} scenarios total, processed {len(docs)} successfully")
                            
            except subprocess.CalledProcessError as e:
                logger.error(f"Failed to clone krkn-hub repository: {e.stderr}")
                raise
            except Exception as e:
                logger.error(f"Error processing krkn-hub repository: {e}")
                raise
                
        return docs
    
    def _process_scenario_json(self, json_path: str, scenario_name: str) -> Dict[str, Any]:
        """Process individual krknctl-input.json file"""
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                parameters = json.load(f)
            
            # Build comprehensive scenario documentation
            content_parts = [
                f"Scenario: {scenario_name}",
                f"Command: krknctl run {scenario_name} [flags]",
                "",
                "Available flags:"
            ]
            
            for param in parameters:
                flag_name = param.get("name", "")
                description = param.get("description", "")
                param_type = param.get("type", "string")
                default = param.get("default", "")
                required = param.get("required", "false")
                
                flag_line = f"--{flag_name}"
                if param_type == "enum" and "allowed_values" in param:
                    allowed = param["allowed_values"]
                    flag_line += f" ({param_type}: {allowed})"
                else:
                    flag_line += f" ({param_type})"
                
                if default:
                    flag_line += f" [default: {default}]"
                if required == "true":
                    flag_line += " [required]"
                
                content_parts.append(f"  {flag_line}")
                if description:
                    content_parts.append(f"    {description}")
                content_parts.append("")
            
            content = "\n".join(content_parts)
            
            return {
                "url": f"krkn-hub://{scenario_name}",
                "title": f"krknctl {scenario_name} scenario",
                "content": content,
                "source": "krkn-hub",
                "scenario_name": scenario_name,
                "parameters": parameters
            }
            
        except Exception as e:
            logger.warning(f"Failed to process scenario JSON {json_path}: {e}")
            return None
    
    def load_krknctl_help(self) -> List[Dict[str, Any]]:
        """Load krknctl help content"""
        docs = []
        
        try:
            help_file = Path(os.path.join(self.home_dir, "krknctl_help.txt"))
            if help_file.exists():
                with open(help_file, 'r') as f:
                    content = f.read()
                    
                docs.append({
                    "url": "krknctl://help",
                    "title": "krknctl Help Documentation", 
                    "content": content,
                    "source": "krknctl"
                })
                    
        except Exception as e:
            logger.warning(f"Failed to load krknctl help: {e}")
            
        return docs
    
    def chunk_documents(self, docs: List[Dict[str, Any]], chunk_size: int = 512) -> List[Dict[str, Any]]:
        """Split documents into smaller chunks for better retrieval"""
        chunked_docs = []
        
        for doc in docs:
            content = doc["content"]
            words = content.split()
            
            # Split into chunks
            for i in range(0, len(words), chunk_size):
                chunk_words = words[i:i + chunk_size]
                chunk_content = " ".join(chunk_words)
                
                if len(chunk_content.strip()) > 50:  # Only keep meaningful chunks
                    chunked_docs.append({
                        "url": doc["url"],
                        "title": doc["title"],
                        "content": chunk_content,
                        "source": doc["source"],
                        "chunk_id": len(chunked_docs)
                    })
                    
        return chunked_docs
    
    def create_embeddings(self, docs: List[Dict[str, Any]]) -> np.ndarray:
        """Create embeddings for document chunks"""
        logger.info(f"Creating embeddings for {len(docs)} document chunks")
        
        texts = [doc["content"] for doc in docs]
        embeddings = self.model.encode(texts, show_progress_bar=True)
        
        return embeddings
    
    def build_faiss_index(self, embeddings: np.ndarray) -> faiss.Index:
        """Build FAISS index for similarity search"""
        dimension = embeddings.shape[1]
        index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity
        
        # Normalize embeddings for cosine similarity
        faiss.normalize_L2(embeddings)
        index.add(embeddings)
        
        return index
    
    def save_index(self, docs: List[Dict[str, Any]], embeddings: np.ndarray, 
                   index: faiss.Index, output_dir: str):
        """Save the complete index to disk"""
        os.makedirs(output_dir, exist_ok=True)
        
        # Save FAISS index
        faiss.write_index(index, os.path.join(output_dir, "index.faiss"))
        
        # Save documents metadata
        with open(os.path.join(output_dir, "documents.json"), 'w') as f:
            json.dump(docs, f, indent=2)
            
        # Save embeddings
        np.save(os.path.join(output_dir, "embeddings.npy"), embeddings)
        
        logger.info(f"Index saved to {output_dir}")
        
    def build_index(self, live_mode: bool = True, output_dir: str = None):
        """Build the complete documentation index"""
        start_time = time.time()
        
        if output_dir is None:
            output_dir = os.path.join(self.home_dir, "docs_index")
            
        # Check if index already exists and remove it
        if os.path.exists(output_dir):
            logger.info(f"Existing index found at {output_dir}, removing it...")
            try:
                shutil.rmtree(output_dir)
                logger.info(f"✅ Existing index removed successfully")
            except Exception as e:
                logger.error(f"❌ Failed to remove existing index: {e}")
                raise
                
        logger.info(f"Building documentation index to {output_dir}...")
        
        # Collect documents
        doc_start = time.time()
        all_docs = []
        
        # Load krknctl help (always available)
        all_docs.extend(self.load_krknctl_help())
        
        # Scrape live documentation if in live mode
        if live_mode:
            github_start = time.time()
            github_docs = self.scrape_krkn_docs()
            all_docs.extend(github_docs)
            github_time = time.time() - github_start
            logger.info(f"📥 GitHub documentation fetched in {github_time:.2f}s")
            
            # Scrape krkn-hub scenario definitions
            hub_start = time.time()
            hub_docs = self.scrape_krkn_hub_scenarios()
            all_docs.extend(hub_docs)
            hub_time = time.time() - hub_start
            logger.info(f"🎯 krkn-hub scenarios fetched in {hub_time:.2f}s")
        else:
            logger.info("Skipping live documentation scraping (offline mode)")
            
        if not all_docs:
            raise Exception("No documents found to index")
            
        doc_time = time.time() - doc_start
        logger.info(f"📄 Found {len(all_docs)} documents in {doc_time:.2f}s")
        
        # Chunk documents
        chunk_start = time.time()
        chunked_docs = self.chunk_documents(all_docs)
        chunk_time = time.time() - chunk_start
        logger.info(f"✂️  Created {len(chunked_docs)} document chunks in {chunk_time:.2f}s")
        
        # Create embeddings
        embed_start = time.time()
        embeddings = self.create_embeddings(chunked_docs)
        embed_time = time.time() - embed_start
        logger.info(f"🧠 Generated embeddings in {embed_time:.2f}s")
        
        # Build FAISS index
        index_start = time.time()
        index = self.build_faiss_index(embeddings)
        index_time = time.time() - index_start
        logger.info(f"🔍 Built FAISS index in {index_time:.2f}s")
        
        # Save everything
        save_start = time.time()
        self.save_index(chunked_docs, embeddings, index, output_dir)
        save_time = time.time() - save_start
        logger.info(f"💾 Saved index in {save_time:.2f}s")
        
        total_time = time.time() - start_time
        logger.info(f"✅ Documentation index built successfully in {total_time:.2f}s total!")

def main():
    parser = argparse.ArgumentParser(description="Build krknctl documentation index")
    parser.add_argument("--home", 
                        default="/app",
                        help="Base directory for RAG service files (default: /app for container, override for local testing)")
    parser.add_argument("--build-cached-index", action="store_true", 
                       help="Build cached index (for container build time)")
    parser.add_argument("--live-index", action="store_true",
                       help="Build live index with fresh documentation")
    parser.add_argument("--output-dir",
                       help="Output directory for the index (overrides default behavior)")
    
    args = parser.parse_args()
    
    # Ensure home directory exists
    os.makedirs(args.home, exist_ok=True)
    
    indexer = DocumentationIndexer(home_dir=args.home)
    
    try:
        if args.build_cached_index:
            # Build cached index (used during container build)
            output_dir = args.output_dir or os.path.join(args.home, "cached_docs")
            indexer.build_index(live_mode=True, output_dir=output_dir)
        elif args.live_index:
            # Build live index (used at runtime)
            output_dir = args.output_dir or os.path.join(args.home, "docs_index")
            indexer.build_index(live_mode=True, output_dir=output_dir)
        else:
            # Default: build live index
            output_dir = args.output_dir or os.path.join(args.home, "docs_index")
            indexer.build_index(live_mode=True, output_dir=output_dir)
            
    except Exception as e:
        logger.error(f"Failed to build index: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()