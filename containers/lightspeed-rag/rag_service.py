#!/usr/bin/env python3
# Generated by Claude Code
# FastAPI RAG service for krknctl Lightspeed

import argparse
import json
import logging
import os
import subprocess
from contextlib import asynccontextmanager
from typing import List, Dict, Any, Optional
from pathlib import Path

import numpy as np
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import faiss
from sentence_transformers import SentenceTransformer
from llama_cpp import Llama

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Global RAG service instance (will be initialized on startup)
rag_service = None

def detect_gpu_type():
    """Detect the type of GPU available and return appropriate llama.cpp parameters"""
    logger.info("Detecting GPU type for optimal llama.cpp backend...")
    
    # Check for NVIDIA GPU devices
    if os.path.exists('/dev/nvidia0') or os.path.exists('/dev/nvidiactl'):
        logger.info("NVIDIA GPU devices detected - using CUDA backend")
        return {
            'backend': 'cuda',
            'n_gpu_layers': -1,  # Use all layers on GPU
            'main_gpu': 0,
            'verbose': True
        }
    
    # Check for DRI devices (Apple Silicon or other GPUs with Vulkan)
    if os.path.exists('/dev/dri/card0') or os.path.exists('/dev/dri/renderD128'):
        logger.info("DRI devices detected - using Vulkan backend")
        return {
            'backend': 'vulkan',
            'n_gpu_layers': -1,  # Use all layers on GPU
            'verbose': True
        }
    
    # Fallback to CPU
    logger.info("No GPU detected - using CPU backend")
    return {
        'backend': 'cpu',
        'n_gpu_layers': 0,  # CPU only
        'n_threads': os.cpu_count(),
        'verbose': True
    }

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Manage application lifespan (startup and shutdown)"""
    global rag_service
    
    # Startup
    logger.info("Starting RAG service...")
    try:
        # Get home directory from global variable set by main
        home_dir = getattr(app.state, 'home_dir', '/app')
        logger.info(f"Using home directory: {home_dir}")
        rag_service = RAGService(home_dir=home_dir)
        rag_service.load_index()
        logger.info("RAG service initialized successfully")
    except Exception as e:
        logger.error(f"Failed to initialize RAG service: {e}")
        raise
    
    yield
    
    # Shutdown (cleanup if needed)
    logger.info("RAG service shutting down...")

app = FastAPI(title="krknctl Lightspeed RAG Service", version="1.0.0", lifespan=lifespan)

class QueryRequest(BaseModel):
    query: str
    max_results: int = 5
    stream: bool = False

class QueryResponse(BaseModel):
    response: str
    sources: List[Dict[str, Any]]
    query: str

class RAGService:
    def __init__(self, home_dir: str = "/app"):
        self.model = None
        self.index = None
        self.documents = []
        self.embeddings = None
        self.llama_model = None
        self.home_dir = home_dir
        # Model path relative to home_dir for flexibility
        self.model_path = os.path.join(home_dir, "models", "Llama-3.2-1B-Instruct-Q4_K_M.gguf")
        
    def load_index(self, index_dir: Optional[str] = None):
        """Load the pre-built documentation index"""
        if index_dir is None:
            index_dir = os.path.join(self.home_dir, "docs_index")
        
        # Ensure the index directory exists
        os.makedirs(index_dir, exist_ok=True)
        logger.info(f"Loading index from {index_dir}")
        
        # Ensure the models directory exists
        models_dir = os.path.dirname(self.model_path)
        os.makedirs(models_dir, exist_ok=True)
        
        try:
            # Load sentence transformer model
            self.model = SentenceTransformer('all-MiniLM-L6-v2')
            
            # Load Llama model
            if os.path.exists(self.model_path):
                logger.info(f"Loading Llama model from {self.model_path}")
                
                # Detect GPU type and get optimal parameters
                gpu_config = detect_gpu_type()
                logger.info(f"Using GPU backend: {gpu_config['backend']}")
                
                # Create Llama model with auto-detected GPU configuration
                llama_params = {
                    'model_path': self.model_path,
                    'n_ctx': 4096,  # Context length
                    'n_gpu_layers': gpu_config['n_gpu_layers'],
                    'verbose': gpu_config['verbose']
                }
                
                # Add backend-specific parameters
                if gpu_config['backend'] == 'cuda':
                    llama_params['main_gpu'] = gpu_config['main_gpu']
                elif gpu_config['backend'] == 'cpu':
                    llama_params['n_threads'] = gpu_config['n_threads']
                
                self.llama_model = Llama(**llama_params)
                
                # Log GPU info after model loading
                logger.info("Llama model loaded successfully")
                logger.info(f"Model using GPU layers: {self.llama_model.n_gpu_layers if hasattr(self.llama_model, 'n_gpu_layers') else 'unknown'}")
                
                # Test a small inference to trigger GPU usage logging
                logger.info("Testing GPU inference...")
                test_response = self.llama_model("Test", max_tokens=1, echo=False)
                logger.info("GPU inference test completed")
            else:
                logger.warning(f"Llama model not found at {self.model_path}")
                logger.info("Model will be downloaded on first use")
            
            # Load FAISS index
            index_path = os.path.join(index_dir, "index.faiss")
            if not os.path.exists(index_path):
                raise FileNotFoundError(f"Index file not found: {index_path}")
            self.index = faiss.read_index(index_path)
            
            # Load documents metadata
            docs_path = os.path.join(index_dir, "documents.json")
            if not os.path.exists(docs_path):
                raise FileNotFoundError(f"Documents file not found: {docs_path}")
            with open(docs_path, 'r') as f:
                self.documents = json.load(f)
                
            # Load embeddings
            embeddings_path = os.path.join(index_dir, "embeddings.npy")
            if os.path.exists(embeddings_path):
                self.embeddings = np.load(embeddings_path)
                
            logger.info(f"Index loaded successfully: {len(self.documents)} documents")
            
        except Exception as e:
            logger.error(f"Failed to load index: {e}")
            raise
            
    def search_documents(self, query: str, max_results: int = 5) -> List[Dict[str, Any]]:
        """Search for relevant documents using semantic similarity with krkn-hub prioritization"""
        if not self.model or not self.index:
            raise RuntimeError("Index not loaded")
            
        # Create query embedding
        query_embedding = self.model.encode([query])
        faiss.normalize_L2(query_embedding)
        
        # Search in FAISS index with much larger result set to ensure we find krkn-hub data
        search_limit = min(len(self.documents), max_results * 10)  # Cast a much wider net
        scores, indices = self.index.search(query_embedding, search_limit)
        
        # Retrieve matching documents with source-based prioritization
        all_results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.documents):
                doc = self.documents[idx].copy()
                doc["relevance_score"] = float(score)
                all_results.append(doc)
        
        # Separate by source
        krkn_hub_results = [doc for doc in all_results if doc.get("source") == "krkn-hub"]
        website_results = [doc for doc in all_results if doc.get("source") != "krkn-hub"]
        
        logger.info(f"Search found {len(krkn_hub_results)} krkn-hub docs, {len(website_results)} website docs")
        
        # For scenario-related queries, aggressively prioritize krkn-hub
        scenario_keywords = ["scenario", "run", "flag", "parameter", "command", "krknctl", "kill", "pod", "node"]
        is_scenario_query = any(keyword in query.lower() for keyword in scenario_keywords)
        
        if is_scenario_query and krkn_hub_results:
            logger.info(f"Scenario query detected, prioritizing {len(krkn_hub_results)} krkn-hub results")
            # Massively boost krkn-hub relevance scores for scenario queries
            for doc in krkn_hub_results:
                doc["relevance_score"] = min(doc["relevance_score"] * 3.0, 1.0)  # Major boost
            
            # Always put krkn-hub results first for scenario queries
            prioritized_results = krkn_hub_results + website_results[:max_results-len(krkn_hub_results)]
        else:
            # For general queries, use normal ordering but still boost krkn-hub slightly
            for doc in krkn_hub_results:
                doc["relevance_score"] = min(doc["relevance_score"] * 1.2, 1.0)
            prioritized_results = all_results
        
        # Sort by relevance score (higher is better) and return top results
        prioritized_results.sort(key=lambda x: x["relevance_score"], reverse=True)
        return prioritized_results[:max_results]
    
    def generate_response(self, query: str, context_docs: List[Dict[str, Any]], 
                         stream: bool = False) -> str:
        """Generate response using llama-cpp-python with RAG context"""
        
        # Build context from retrieved documents
        context_parts = []
        for doc in context_docs:
            source = f"Source: {doc['source']} - {doc['title']}"
            content = doc['content'][:500]  # Limit context length
            context_parts.append(f"{source}\n{content}")
            
        context = "\n\n---\n\n".join(context_parts)
        
        # Create prompt for krknctl-specific assistance
        prompt = f"""You are a helpful krknctl assistant for chaos engineering scenarios.

COMMAND SYNTAX: krknctl run <scenario_name> <scenario_flags>

PRIORITY: When you see "source: krkn-hub" in the context, that is AUTHORITATIVE scenario data with real flags. Use it over website docs.

Context with scenario definitions:
{context}

User Question: {query}

GUIDELINES:
1. Help users understand krknctl scenarios and provide accurate commands
2. ALWAYS prioritize "source: krkn-hub" data when available - it contains the real flags
3. Extract user context (pod names, namespaces, labels) into actual flag names 
4. Use only real flags from the authoritative sources
5. If asked about non-krknctl topics, politely redirect to krknctl scenarios
6. Be helpful and explain what scenarios do

RESPONSE FORMAT:
🎯 [Brief description of what this does]
┌─────────────────────────────────────────────────────┐
│ krknctl run scenario-name --flag=value             │
└─────────────────────────────────────────────────────┘

Answer:"""

        try:
            if not self.llama_model:
                return "Llama model not loaded. Please check the model path and try again."
                
            if stream:
                # Return streaming generator for real-time responses
                response_generator = self.llama_model(
                    prompt,
                    max_tokens=512,
                    temperature=0.7,
                    top_p=0.9,
                    stream=True
                )
                return response_generator
            else:
                # Return complete response
                response = self.llama_model(
                    prompt,
                    max_tokens=512,
                    temperature=0.7,
                    top_p=0.9,
                    echo=False
                )
                return response['choices'][0]['text'].strip()
                
        except Exception as e:
            logger.error(f"Error generating response: {e}")
            return f"I encountered an error while processing your request. Please try again or check if the model is properly loaded."


@app.get("/health")
async def health_check():
    """Health check endpoint"""
    model_status = "loaded" if rag_service.llama_model else "not_loaded"
    model_path = os.path.basename(rag_service.model_path) if rag_service.model_path else "unknown"
    
    return {
        "status": "healthy",
        "service": "krknctl-lightspeed-rag",
        "model": f"llama-cpp-python:{model_path}",
        "model_status": model_status,
        "documents_indexed": len(rag_service.documents) if rag_service.documents else 0
    }

@app.post("/query", response_model=QueryResponse)
async def query_rag(request: QueryRequest):
    """Query the RAG system for krknctl assistance"""
    try:
        if not rag_service.model or not rag_service.index:
            raise HTTPException(status_code=503, detail="RAG service not initialized")
            
        # Search for relevant documents
        relevant_docs = rag_service.search_documents(request.query, request.max_results)
        
        if not relevant_docs:
            raise HTTPException(status_code=404, detail="No relevant documentation found")
            
        # Generate response
        if request.stream:
            # Handle streaming separately
            raise HTTPException(status_code=400, detail="Use /query/stream for streaming responses")
            
        response_text = rag_service.generate_response(request.query, relevant_docs, stream=False)
        
        # Prepare sources for response
        sources = []
        for doc in relevant_docs:
            sources.append({
                "title": doc.get("title", ""),
                "url": doc.get("url", ""),
                "source": doc.get("source", ""),
                "relevance_score": doc.get("relevance_score", 0.0)
            })
            
        return QueryResponse(
            response=response_text,
            sources=sources,
            query=request.query
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error processing query: {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

@app.post("/query/stream")
async def query_rag_stream(request: QueryRequest):
    """Query the RAG system with streaming response"""
    try:
        if not rag_service.model or not rag_service.index:
            raise HTTPException(status_code=503, detail="RAG service not initialized")
            
        # Search for relevant documents
        relevant_docs = rag_service.search_documents(request.query, request.max_results)
        
        if not relevant_docs:
            raise HTTPException(status_code=404, detail="No relevant documentation found")
            
        # Generate streaming response
        def generate_stream():
            try:
                response_stream = rag_service.generate_response(request.query, relevant_docs, stream=True)
                for chunk in response_stream:
                    # llama-cpp-python streaming format: chunk['choices'][0]['text']
                    if chunk.get('choices') and len(chunk['choices']) > 0:
                        text = chunk['choices'][0].get('text', '')
                        if text:
                            yield f"data: {json.dumps({'text': text})}\n\n"
                        
                # Send sources at the end
                sources = []
                for doc in relevant_docs:
                    sources.append({
                        "title": doc.get("title", ""),
                        "url": doc.get("url", ""),
                        "source": doc.get("source", ""),
                        "relevance_score": doc.get("relevance_score", 0.0)
                    })
                yield f"data: {json.dumps({'sources': sources, 'query': request.query})}\n\n"
                yield "data: [DONE]\n\n"
                
            except Exception as e:
                yield f"data: {json.dumps({'error': str(e)})}\n\n"
                
        return StreamingResponse(
            generate_stream(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error processing streaming query: {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

@app.get("/")
async def root():
    """Root endpoint with service information"""
    return {
        "service": "krknctl Lightspeed RAG",
        "version": "1.0.0",
        "description": "Retrieval-Augmented Generation service for krknctl chaos engineering assistance",
        "endpoints": {
            "health": "/health",
            "query": "/query",
            "stream": "/query/stream"
        }
    }

def parse_args():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description='krknctl Lightspeed RAG Service')
    parser.add_argument('--home', 
                        default='/app',
                        help='Base directory for RAG service files (default: /app for container, override for local testing)')
    parser.add_argument('--host',
                        default='0.0.0.0',
                        help='Host to bind the service to (default: 0.0.0.0)')
    parser.add_argument('--port',
                        type=int,
                        default=8080,
                        help='Port to bind the service to (default: 8080)')
    return parser.parse_args()

if __name__ == "__main__":
    args = parse_args()
    
    # Ensure home directory exists
    os.makedirs(args.home, exist_ok=True)
    
    # Set home directory in app state for lifespan to use
    app.state.home_dir = args.home
    
    logger.info(f"Starting RAG service with home directory: {args.home}")
    
    import uvicorn
    uvicorn.run(app, host=args.host, port=args.port)