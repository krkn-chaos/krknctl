# Generated by Claude Code
# krknctl Lightspeed RAG Container with llama-cpp-python + GPU acceleration for macOS/Podman
FROM registry.fedoraproject.org/fedora:40

# Install all system dependencies in one layer (multi-backend support: CUDA + Vulkan)
RUN dnf -y install dnf-plugins-core && \
    dnf -y install dnf-plugin-versionlock && \
    dnf -y install \
    python3 \
    python3-pip \
    python3-devel \
    curl \
    bash \
    tar \
    gzip \
    git \
    cmake \
    gcc \
    gcc-c++ \
    make \
    wget \
    which \
    # Vulkan support (for Apple Silicon)
    mesa-vulkan-drivers \
    vulkan-loader-devel \
    vulkan-headers \
    vulkan-tools \
    vulkan-loader \
    glslc \
    && (dnf -y copr enable slp/mesa-krunkit fedora-40-aarch64 || echo "Mesa krunkit copr not available, skipping") \
    && (dnf -y downgrade mesa-vulkan-drivers.aarch64 --repo=copr:copr.fedorainfracloud.org:slp:mesa-krunkit || echo "Mesa downgrade not available, skipping") \
    && (dnf versionlock mesa-vulkan-drivers || echo "Mesa versionlock not available, skipping") \
    && dnf clean all


# Verify glslc and GPU setup
RUN which glslc && glslc --version && \
    ls -la /usr/share/vulkan/icd.d/

# Set working directory
WORKDIR /app

# Install krkn-lightspeed requirements (except llama-cpp-python which we build separately)
RUN git clone https://github.com/krkn-chaos/krkn-lightspeed.git /tmp/krkn-lightspeed && \
    cd /tmp/krkn-lightspeed && \
    git checkout krknctl_lightspeed && \
    pip3 install --no-cache-dir -r requirements.txt && \
    rm -rf /tmp/krkn-lightspeed

# Compile and install llama-cpp-python with Vulkan support (Apple Silicon optimized)
RUN CMAKE_ARGS="-DGGML_VULKAN=on" pip3 install --no-cache-dir --verbose llama-cpp-python

# Install huggingface-hub for reliable model downloads
RUN pip3 install --no-cache-dir huggingface-hub

# Download LLM and embedding models
RUN mkdir -p /app/models && \
    python3 -c "import huggingface_hub; huggingface_hub.hf_hub_download(repo_id='bartowski/Llama-3.2-1B-Instruct-GGUF', filename='Llama-3.2-1B-Instruct-Q4_K_M.gguf', local_dir='/app/models', local_dir_use_symlinks=False)" && \
    ls -la /app/models/ && \
    [ $(stat -c%s /app/models/Llama-3.2-1B-Instruct-Q4_K_M.gguf) -gt 500000000 ] || (echo "Model download failed - file too small" && exit 1)

# Pre-download embedding models to avoid runtime downloads
RUN mkdir -p /root/.cache/huggingface/transformers && \
    python3 -c "from sentence_transformers import SentenceTransformer; print('Downloading Qwen embedding model...'); model = SentenceTransformer('Qwen/Qwen3-Embedding-0.6B'); print('Qwen model cached successfully'); print('Downloading fallback embedding model...'); fallback = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2'); print('Fallback model cached successfully')" && \
    echo "Embedding models pre-downloaded successfully"

# Clone krkn-lightspeed repository and checkout krknctl_lightspeed branch
RUN git clone https://github.com/krkn-chaos/krkn-lightspeed.git /app/krkn-lightspeed && \
    cd /app/krkn-lightspeed && \
    git checkout krknctl_lightspeed && \
    echo "Repository cloned and checked out successfully"

# Copy krknctl help for documentation indexing
COPY krknctl_help.txt .
COPY entrypoint.sh .

# Make scripts executable
RUN chmod +x entrypoint.sh

# Create directories
RUN mkdir -p /app/docs_index /app/cached_docs

# Pre-build cached documentation index using the cloned repository
RUN cd /app/krkn-lightspeed && \
    echo "Documentation index preparation completed"

# Expose port for FastAPI service
EXPOSE 8080

# Set environment variables
ENV PYTHONUNBUFFERED=1

# Add NVIDIA Vulkan support (optional - will skip if not available)
RUN (dnf -y install akmod-nvidia xorg-x11-drv-nvidia-cuda || echo "NVIDIA drivers not available, skipping") && \
    (ls -la /usr/share/vulkan/icd.d/ || echo "Vulkan ICD check")

# Run the entrypoint script
ENTRYPOINT ["./entrypoint.sh"]