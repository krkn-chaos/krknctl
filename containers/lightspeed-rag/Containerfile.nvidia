# Generated by Claude Sonnet 4
# krknctl Lightspeed RAG Container - NVIDIA GPU optimized (x86_64) - Multi-stage build
FROM registry.fedoraproject.org/fedora-minimal:42 AS builder

# Install build dependencies
RUN microdnf -y install \
    python3 \
    python3-pip \
    python3-devel \
    curl \
    bash \
    git \
    wget \
    which \
    gcc \
    gcc-c++ \
    cmake \
    make \
    && microdnf clean all

# Set working directory
WORKDIR /app

# Create and activate virtual environment
RUN python3 -m venv /app/venv
ENV PATH="/app/venv/bin:$PATH"

# Install krkn-lightspeed requirements (except llama-cpp-python which we build separately)
RUN git clone https://github.com/krkn-chaos/krkn-lightspeed.git /tmp/krkn-lightspeed && \
    cd /tmp/krkn-lightspeed && \
    git checkout krknctl_lightspeed && \
    /app/venv/bin/pip install --no-cache-dir -r requirements.txt && \
    rm -rf /tmp/krkn-lightspeed

# Install PyTorch with CUDA support (override CPU-only version from requirements.txt)
RUN /app/venv/bin/pip install --no-cache-dir --force-reinstall torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118

# Install llama-cpp-python (standard version from PyPI)
RUN /app/venv/bin/pip install --no-cache-dir llama-cpp-python

# Install huggingface-hub for reliable model downloads
RUN /app/venv/bin/pip install --no-cache-dir huggingface-hub

# Pre-download embedding models to avoid runtime downloads
RUN mkdir -p /root/.cache/huggingface/transformers && \
    /app/venv/bin/python -c "from sentence_transformers import SentenceTransformer; print('Downloading Qwen embedding model...'); model = SentenceTransformer('Qwen/Qwen3-Embedding-0.6B'); print('Qwen model cached successfully'); print('Downloading fallback embedding model...'); fallback = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2'); print('Fallback model cached successfully')" && \
    echo "Embedding models pre-downloaded successfully"

# Careful cleanup: remove pip cache and unnecessary files but preserve numpy core tests
RUN find /app/venv -name "*.pyc" -delete && \
    find /app/venv -name "__pycache__" -delete && \
    find /app/venv -name "*.pyo" -delete && \
    find /app/venv -name "*.egg-info" -type d -exec rm -rf {} + 2>/dev/null || true && \
    rm -rf /app/venv/lib/python*/site-packages/torch/test && \
    rm -rf /app/venv/lib/python*/site-packages/torch/share && \
    rm -rf /app/venv/lib/python*/site-packages/transformers/tests && \
    rm -rf /app/venv/lib/python*/site-packages/sklearn/tests && \
    rm -rf /app/venv/lib/python*/site-packages/scipy/*/tests && \
    rm -rf /tmp/* 2>/dev/null || true

# Verify that numpy and sentence-transformers work correctly after cleanup
RUN /app/venv/bin/python -c "import numpy; print(f'numpy version: {numpy.__version__}')" && \
    /app/venv/bin/python -c "import sentence_transformers; print(f'sentence-transformers version: {sentence_transformers.__version__}')"

# Download LLM and embedding models in builder stage
RUN mkdir -p /app/models && \
    /app/venv/bin/python -c "import huggingface_hub; huggingface_hub.hf_hub_download(repo_id='bartowski/Llama-3.2-1B-Instruct-GGUF', filename='Llama-3.2-1B-Instruct-Q4_K_M.gguf', local_dir='/app/models', local_dir_use_symlinks=False)" && \
    ls -la /app/models/ && \
    [ $(stat -c%s /app/models/Llama-3.2-1B-Instruct-Q4_K_M.gguf) -gt 500000000 ] || (echo "Model download failed - file too small" && exit 1)

# Copy krknctl help for documentation indexing
COPY krknctl_help.txt .

#############################################
# Runtime stage - minimal runtime dependencies
#############################################
FROM registry.fedoraproject.org/fedora-minimal:42 AS runtime

# Install only runtime dependencies
RUN microdnf -y install \
    python3 \
    bash \
    curl \
    nvidia-container-toolkit \
    && microdnf clean all

# Set working directory
WORKDIR /app

# Copy the complete virtual environment from builder
COPY --from=builder /app/venv/ /app/venv/

# Activate virtual environment for all subsequent commands
ENV PATH="/app/venv/bin:$PATH"
ENV VIRTUAL_ENV="/app/venv"

# Copy application files and pre-built models from builder (but NOT the source code)
COPY --from=builder /app/models/ /app/models/
COPY --from=builder /app/krknctl_help.txt /app/krknctl_help.txt
COPY --from=builder /root/.cache/huggingface/ /root/.cache/huggingface/

# Create docs_index directory for runtime index generation
RUN mkdir -p /app/docs_index /app/cached_docs

# Copy entrypoint script
COPY entrypoint.sh .

# Make scripts executable
RUN chmod +x entrypoint.sh

# Install git in runtime stage for documentation indexing
RUN microdnf -y install git && microdnf clean all

# Expose port for FastAPI service
EXPOSE 8080

# Set environment variables
ENV PYTHONUNBUFFERED=1

# Set CUDA environment variables for automatic GPU detection by PyTorch/sentence-transformers
ENV CUDA_VISIBLE_DEVICES=0
ENV TORCH_USE_CUDA_DSA=1
ENV PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128

# CACHE INVALIDATION POINT: Change this ARG to force fresh code checkout
ARG CODE_VERSION=latest
RUN echo "Building with code version: $CODE_VERSION"

# Fresh clone of krkn-lightspeed repository (this layer will be invalidated when CODE_VERSION changes)
RUN git clone https://github.com/krkn-chaos/krkn-lightspeed.git /app/krkn-lightspeed && \
    cd /app/krkn-lightspeed && \
    git checkout krknctl_lightspeed && \
    echo "Repository cloned and checked out successfully at version: $CODE_VERSION"

# Run the entrypoint script
ENTRYPOINT ["./entrypoint.sh"]