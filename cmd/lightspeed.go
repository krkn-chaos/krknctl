// Generated by Claude Code
// RAG model deployment functions for krknctl lightspeed

package cmd

import (
	"bufio"
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"os"
	"os/signal"
	"strings"
	"syscall"
	"time"

	"github.com/krkn-chaos/krknctl/pkg/config"
	"github.com/krkn-chaos/krknctl/pkg/gpucheck"
	"github.com/krkn-chaos/krknctl/pkg/provider/models"
	"github.com/krkn-chaos/krknctl/pkg/scenarioorchestrator"
)

// RAGDeploymentResult holds information about the deployed RAG model
type RAGDeploymentResult struct {
	containerID string
	hostPort    string
}

// deployRAGModelWithGPUType deploys the RAG model container using the new GPU detection system
func deployRAGModelWithGPUType(ctx context.Context, gpuType gpucheck.GPUAcceleration, offline bool, orchestrator scenarioorchestrator.ScenarioOrchestrator, config config.Config, registry *models.RegistryV2, detector *gpucheck.PlatformGPUDetector) (*RAGDeploymentResult, error) {
	// Get the appropriate lightspeed image for the detected GPU type
	ragImageURI, err := detector.GetLightspeedImageURI(gpuType)
	if err != nil {
		return nil, fmt.Errorf("failed to get lightspeed image URI: %w", err)
	}

	// Generate unique container name using config
	containerName := fmt.Sprintf("%s-%d", config.RAGContainerPrefix, time.Now().Unix())

	// Set up environment variables
	env := map[string]string{}
	if offline {
		env["USE_OFFLINE"] = "true"
	} else {
		env["USE_OFFLINE"] = "false"
	}

	// Get device mounts from detector
	devices := detector.GetDeviceMounts(gpuType)

	// Set up port mapping using config
	hostPort := config.RAGServicePort      // Host port (e.g., "8080")
	containerPort := config.RAGServicePort // Container port (e.g., "8080")
	portMappings := &map[string]string{
		hostPort: containerPort, // host port -> container port
	}

	// Create spinner for pull progress (exactly like run.go)
	spinner := NewSpinnerWithSuffix("pulling RAG model image...")
	spinner.Start()

	// Create communication channel for pull progress updates
	commChan := make(chan *string)
	go func() {
		for msg := range commChan {
			spinner.Suffix = *msg
		}
		spinner.Stop()
	}()

	// Run the RAG container in detached mode
	containerID, err := orchestrator.Run(ragImageURI, containerName, env, false, nil, &devices,
		&commChan, ctx, registry, portMappings)

	// The orchestrator closes the channel automatically, so we don't need to close it manually
	if err != nil {
		return nil, fmt.Errorf("failed to run RAG container: %w", err)
	}
	fmt.Printf("üöÄ RAG container started: %s\n", *containerID)
	fmt.Printf("üì° Port mapping: %s:%s -> container:%s\n", config.RAGHost, hostPort, config.RAGServicePort)

	return &RAGDeploymentResult{
		containerID: *containerID,
		hostPort:    hostPort,
	}, nil
}

// deployRAGModel deploys the RAG model container in detached mode with proper port mapping
// DEPRECATED: Use deployRAGModelWithGPUType instead
func deployRAGModel(ctx context.Context, gpuType string, offline bool, orchestrator scenarioorchestrator.ScenarioOrchestrator, config config.Config, registry *models.RegistryV2) (*RAGDeploymentResult, error) {
	// Get RAG model image URI
	ragImageURI, err := config.GetRAGModelImageURI()
	if err != nil {
		return nil, fmt.Errorf("failed to get RAG model image URI: %w", err)
	}

	// Generate unique container name using config
	containerName := fmt.Sprintf("%s-%d", config.RAGContainerPrefix, time.Now().Unix())

	// Set up environment variables
	env := map[string]string{}
	if offline {
		env["USE_OFFLINE"] = "true"
	} else {
		env["USE_OFFLINE"] = "false"
	}

	// Set up GPU device mounts (legacy approach)
	devices := make(map[string]string)
	switch gpuType {
	case "nvidia":
		devices["/dev/nvidia0"] = "/dev/nvidia0"
		devices["/dev/nvidiactl"] = "/dev/nvidiactl"
		devices["/dev/nvidia-uvm"] = "/dev/nvidia-uvm"
	case "apple-silicon":
		devices["/dev/dri"] = "/dev/dri"
	}

	// Set up port mapping using config
	hostPort := config.RAGServicePort      // Host port (e.g., "8080")
	containerPort := config.RAGServicePort // Container port (e.g., "8080")
	portMappings := &map[string]string{
		hostPort: containerPort, // host port -> container port
	}

	// Create spinner for pull progress (exactly like run.go)
	spinner := NewSpinnerWithSuffix("pulling RAG model image...")
	spinner.Start()

	// Create communication channel for pull progress updates
	commChan := make(chan *string)
	go func() {
		for msg := range commChan {
			spinner.Suffix = *msg
		}
		spinner.Stop()
	}()

	// Run the RAG container in detached mode
	containerID, err := orchestrator.Run(ragImageURI, containerName, env, false, nil, &devices,
		&commChan, ctx, registry, portMappings)

	// The orchestrator closes the channel automatically, so we don't need to close it manually
	if err != nil {
		return nil, fmt.Errorf("failed to run RAG container: %w", err)
	}
	fmt.Printf("üöÄ RAG container started: %s\n", *containerID)
	fmt.Printf("üì° Port mapping: %s:%s -> container:%s\n", config.RAGHost, hostPort, config.RAGServicePort)

	return &RAGDeploymentResult{
		containerID: *containerID,
		hostPort:    hostPort,
	}, nil
}


// HealthResponse represents the health check response
type HealthResponse struct {
	Status           string `json:"status"`
	Service          string `json:"service"`
	Model            string `json:"model"`
	DocumentsIndexed int    `json:"documents_indexed"`
}

// performRAGHealthCheck performs health checking with timeout and cleanup on failure
func performRAGHealthCheck(containerID string, hostPort string, orchestrator scenarioorchestrator.ScenarioOrchestrator, ctx context.Context, config config.Config) (bool, error) {
	// Health URL is constructed from trusted config values and validated port
	healthURL := fmt.Sprintf("http://%s:%s%s", config.RAGHost, hostPort, config.RAGHealthEndpoint)
	maxRetries := config.RAGHealthMaxRetries
	retryInterval := time.Duration(config.RAGHealthRetryIntervalSeconds) * time.Second

	fmt.Printf("ü©∫ Health checking Lightspeed service at %s...\n", healthURL)

	for i := 0; i < maxRetries; i++ {
		// Check if container is still running
		containers, err := orchestrator.ListRunningContainers(ctx)
		if err != nil {
			fmt.Printf("‚ö†Ô∏è  Warning: failed to list containers: %v\n", err)
		} else {
			containerFound := false
			if containers != nil {
				for _, container := range *containers {
					if container.ID == containerID {
						containerFound = true
						break
					}
				}
			}
			if !containerFound {
				return false, fmt.Errorf("container %s is no longer running", containerID)
			}
		}

		// Perform health check - URL is safe as it's constructed from config
		// #nosec G107 - URL constructed from trusted configuration
		resp, err := http.Get(healthURL)
		if err == nil && resp.StatusCode == 200 {
			var health HealthResponse
			if err := json.NewDecoder(resp.Body).Decode(&health); err == nil {
				if closeErr := resp.Body.Close(); closeErr != nil {
					fmt.Printf("‚ö†Ô∏è  Warning: failed to close response body: %v\n", closeErr)
				}
				if health.Status == "healthy" {
					fmt.Printf("‚úÖ Service healthy: %s with %d documents indexed\n", health.Model, health.DocumentsIndexed)
					return true, nil
				}
			}
			if closeErr := resp.Body.Close(); closeErr != nil {
				fmt.Printf("‚ö†Ô∏è  Warning: failed to close response body: %v\n", closeErr)
			}
		}

		if i < maxRetries-1 {
			fmt.Printf("‚è≥ Waiting for service to become ready... (%d/%d)\n", i+1, maxRetries)
			time.Sleep(retryInterval)
		}
	}

	// Health check failed, clean up the container
	timeoutMinutes := (maxRetries * config.RAGHealthRetryIntervalSeconds) / 60
	fmt.Printf("‚ùå Health check timed out after %d minutes, cleaning up container...\n", timeoutMinutes)
	if err := orchestrator.Kill(&containerID, ctx); err != nil {
		fmt.Printf("‚ö†Ô∏è  Warning: failed to clean up container %s: %v\n", containerID, err)
	}

	return false, fmt.Errorf("service did not become healthy within timeout")
}

// QueryRequest represents a request to the Lightspeed service
type QueryRequest struct {
	Query      string `json:"query"`
	MaxResults int    `json:"max_results"`
	Stream     bool   `json:"stream"`
}

// QueryResponse represents a response from the Lightspeed service
type QueryResponse struct {
	Response string                   `json:"response"`
	Sources  []map[string]interface{} `json:"sources"`
	Query    string                   `json:"query"`
}

// startInteractivePrompt starts an interactive chat session with the Lightspeed service
func startInteractivePrompt(containerID string, hostPort string, orchestrator scenarioorchestrator.ScenarioOrchestrator, ctx context.Context, config config.Config) error {
	scanner := bufio.NewScanner(os.Stdin)

	// Set up signal handling for Ctrl+C
	signalChan := make(chan os.Signal, 1)
	signal.Notify(signalChan, os.Interrupt, syscall.SIGTERM)

	// Handle Ctrl+C in a goroutine
	go func() {
		<-signalChan
		fmt.Println("\n\nüõë Interrupted! Cleaning up Lightspeed service...")
		if err := orchestrator.Kill(&containerID, ctx); err != nil {
			fmt.Printf("‚ö†Ô∏è  Warning: failed to stop Lightspeed container: %v\n", err)
		} else {
			fmt.Println("‚úÖ Lightspeed service stopped successfully")
		}
		os.Exit(0)
	}()

	fmt.Printf("ü§ñ AI Assistant ready! Ask me about krknctl commands or chaos engineering:\n")
	fmt.Printf("üìç Service available at: http://%s:%s\n", config.RAGHost, hostPort)
	fmt.Printf("üí° Try asking: 'How do I run a pod deletion scenario?'\n")
	fmt.Printf("üö™ Type 'exit', 'quit', or press Ctrl+C to stop.\n\n")

	for {
		fmt.Print("> ")
		if !scanner.Scan() {
			break
		}

		query := strings.TrimSpace(scanner.Text())
		if query == "" {
			continue
		}

		if strings.ToLower(query) == "exit" || strings.ToLower(query) == "quit" {
			fmt.Println("üëã Goodbye!")
			break
		}

		// Send query to Lightspeed service
		response, err := queryRAGService(hostPort, query, config)
		if err != nil {
			fmt.Printf("‚ùå Error: %v\n", err)
			fmt.Printf("üí° Tip: Make sure the service is fully initialized. Large models may take a few minutes.\n")
			continue
		}

		// Display response
		fmt.Printf("\nü§ñ %s\n", response.Response)
		fmt.Println()
	}

	if err := scanner.Err(); err != nil {
		return fmt.Errorf("error reading input: %w", err)
	}

	// Clean up: stop the container
	fmt.Println("\nüßπ Cleaning up Lightspeed service...")
	if err := orchestrator.Kill(&containerID, ctx); err != nil {
		fmt.Printf("‚ö†Ô∏è  Warning: failed to stop Lightspeed container: %v\n", err)
	} else {
		fmt.Println("‚úÖ Lightspeed service stopped successfully")
	}

	return nil
}

// queryRAGService sends a query to the Lightspeed service and returns the response
func queryRAGService(hostPort string, query string, config config.Config) (*QueryResponse, error) {
	// Query URL is constructed from trusted config values and validated port
	url := fmt.Sprintf("http://%s:%s%s", config.RAGHost, hostPort, config.RAGQueryEndpoint)

	requestBody := QueryRequest{
		Query:      query,
		MaxResults: config.RAGQueryMaxResults,
		Stream:     false,
	}

	jsonData, err := json.Marshal(requestBody)
	if err != nil {
		return nil, fmt.Errorf("failed to marshal request: %w", err)
	}

	// #nosec G107 - URL constructed from trusted configuration
	resp, err := http.Post(url, "application/json", bytes.NewBuffer(jsonData))
	if err != nil {
		return nil, fmt.Errorf("failed to send request: %w", err)
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		body, _ := io.ReadAll(resp.Body)
		return nil, fmt.Errorf("service returned error %d: %s", resp.StatusCode, string(body))
	}

	var response QueryResponse
	if err := json.NewDecoder(resp.Body).Decode(&response); err != nil {
		return nil, fmt.Errorf("failed to decode response: %w", err)
	}

	return &response, nil
}
